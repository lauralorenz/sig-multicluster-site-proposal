{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Introduction \u00b6 Since its inception in 2015, Kubernetes has been pretty successful at popularizing the idea of container clusters. Adoption has reached the point that many users are deploying their applications across multiple clusters and are struggling to make it work smoothly. SIG-Multicluster is a Special Interest Group focused on solving common challenges related to the management of multiple Kubernetes clusters, and applications that exist therein. Specifically, the SIG aims to fine Kubernetes-native ways to: Expose workloads from multiple clusters to each other Share cluster metadata and its place relative to others Generally break down the walls between clusters The SIG is responsible for designing, discussing, implementing and maintaining API\u2019s, tools and documentation related to multi-cluster administration and application management. This includes not only active automated approaches such as Cluster Federation, but also those that employ batch workflow-style continuous deployment systems like Spinnaker and others. Standalone building blocks for these and other similar systems (for example a cluster registry), and proposed changes to kubernetes core where appropriate will also be in scope. Problem statement: why multicluster? \u00b6 There are many reasons to want to run multiple clusters, including but not limited to: Location Latency: it can be necessary to deploy the application as close to the customers as possible. Jurisdiction: it can be mandated to keep user data in-country. Data gravity: data already exists in one provider, but it can be decided to run the application in another environment. Isolation Environment (e.g. dev, test, prod) Performance isolation: a workload may consume too many resources, at the expense of other workloads. Security isolation: sensitive data or untrusted code must be isolated in their own environments. Organizational isolation: teams may have different management domains. Cost isolation: multitenancy can greatly complexify billing management for different teams. Reliability Blast radius: an infrastructure or application incident in one cluster must not impact the whole system. Infrastructure diversity: an underlying zone, region, or provider outage does not bring down the whole system. Scale: the application is too big to fit in a single cluster. Upgrade scope: some parts of the application may require an infrastructure upgrade, that may impact other parts of the application. Having multiple clusters can also avoid the need for in-place cluster upgrades. Project charter \u00b6 The following charter defines the scope and governance of the Multicluster Special Interest Group : Support an out-of-tree ecosystem by defining API standards that leave plenty of rool for flexibility in their ultimately third party implementation. Protect the known multicluster experience from incompatible changes upstream to single clusters by working with other SIGs . Advocate and provide feedback on proposals for extending the single cluster experience to multicluster i.e. network policy. Approach \u00b6 To meet the goals listed above, the SIG-Multicluster team has worked to define three different APIs: About API : allows to uniquely identify clusters within a set of clusters ( clusterset ) Multicluster Services API : allows to expose services across clusters which are part of a given clusterset . Work API : allows to define the workloads to be deployed across clusters which are part of a given clusterset . To leave room for implementation, SIG-Multicluster does not focus on the implementation of the mechanisms that rely on those APIs. For example, no reference implementation is provided for a cluster registry or for service discovery itself. Getting started \u00b6 Whether you are a user interested in using the different APIs or an implementer interested in conforming to the APIs, the following resources will help give you the necessary background: KubeCon NA 2022 \"SIG-Multicluster Intro and Deep Dive\" by Laura Lorenz (Google), Jeremy Olmsted-Thompson (Google) and Paul Morie (Apple) Implementation guide and references Community links Contributing \u00b6 If you are interested in contributing to SIG-Multicluster or building an implementation of one of our APIs, then don\u2019t hesitate to get involved in SIG meetings, issues on projects, or new designs.","title":"Introduction"},{"location":"#introduction","text":"Since its inception in 2015, Kubernetes has been pretty successful at popularizing the idea of container clusters. Adoption has reached the point that many users are deploying their applications across multiple clusters and are struggling to make it work smoothly. SIG-Multicluster is a Special Interest Group focused on solving common challenges related to the management of multiple Kubernetes clusters, and applications that exist therein. Specifically, the SIG aims to fine Kubernetes-native ways to: Expose workloads from multiple clusters to each other Share cluster metadata and its place relative to others Generally break down the walls between clusters The SIG is responsible for designing, discussing, implementing and maintaining API\u2019s, tools and documentation related to multi-cluster administration and application management. This includes not only active automated approaches such as Cluster Federation, but also those that employ batch workflow-style continuous deployment systems like Spinnaker and others. Standalone building blocks for these and other similar systems (for example a cluster registry), and proposed changes to kubernetes core where appropriate will also be in scope.","title":"Introduction"},{"location":"#problem-statement-why-multicluster","text":"There are many reasons to want to run multiple clusters, including but not limited to: Location Latency: it can be necessary to deploy the application as close to the customers as possible. Jurisdiction: it can be mandated to keep user data in-country. Data gravity: data already exists in one provider, but it can be decided to run the application in another environment. Isolation Environment (e.g. dev, test, prod) Performance isolation: a workload may consume too many resources, at the expense of other workloads. Security isolation: sensitive data or untrusted code must be isolated in their own environments. Organizational isolation: teams may have different management domains. Cost isolation: multitenancy can greatly complexify billing management for different teams. Reliability Blast radius: an infrastructure or application incident in one cluster must not impact the whole system. Infrastructure diversity: an underlying zone, region, or provider outage does not bring down the whole system. Scale: the application is too big to fit in a single cluster. Upgrade scope: some parts of the application may require an infrastructure upgrade, that may impact other parts of the application. Having multiple clusters can also avoid the need for in-place cluster upgrades.","title":"Problem statement: why multicluster?"},{"location":"#project-charter","text":"The following charter defines the scope and governance of the Multicluster Special Interest Group : Support an out-of-tree ecosystem by defining API standards that leave plenty of rool for flexibility in their ultimately third party implementation. Protect the known multicluster experience from incompatible changes upstream to single clusters by working with other SIGs . Advocate and provide feedback on proposals for extending the single cluster experience to multicluster i.e. network policy.","title":"Project charter"},{"location":"#approach","text":"To meet the goals listed above, the SIG-Multicluster team has worked to define three different APIs: About API : allows to uniquely identify clusters within a set of clusters ( clusterset ) Multicluster Services API : allows to expose services across clusters which are part of a given clusterset . Work API : allows to define the workloads to be deployed across clusters which are part of a given clusterset . To leave room for implementation, SIG-Multicluster does not focus on the implementation of the mechanisms that rely on those APIs. For example, no reference implementation is provided for a cluster registry or for service discovery itself.","title":"Approach"},{"location":"#getting-started","text":"Whether you are a user interested in using the different APIs or an implementer interested in conforming to the APIs, the following resources will help give you the necessary background: KubeCon NA 2022 \"SIG-Multicluster Intro and Deep Dive\" by Laura Lorenz (Google), Jeremy Olmsted-Thompson (Google) and Paul Morie (Apple) Implementation guide and references Community links","title":"Getting started"},{"location":"#contributing","text":"If you are interested in contributing to SIG-Multicluster or building an implementation of one of our APIs, then don\u2019t hesitate to get involved in SIG meetings, issues on projects, or new designs.","title":"Contributing"},{"location":"api-types/cluster-set/","text":"ClusterSet \u00b6 ClusterSet represents a specific pattern implemented by various organizations. A ClusterSet is typically: A group of clusters governed by a single authority. There is usually a high degree of trust within the set of clusters. Namespace Sameness applies to clusters in the set: Permissions and characteristics are consistent across clusters for a given namespace. Namespaces don't have to exist in every cluster, but behave the same across those in which they do. Note The early definition of the ClusterSet was described in KEP-2149 . It is now part of the About API . A cluster's ClusterSet membership is stored in the about.k8s.io/ClusterProperty clusterset.k8s.io . Cluster Metadata \u00b6 The ClusterSet is a Cluster-scoped ClusterProperty CRD (Customer Resource Definition), that stores a name and a value. This property can be used to: uniquely identify clusters using a clusterID apiVersion: about.k8s.io/v1 kind: ClusterProperty metadata: name: cluster.clusterset.k8s.io spec: value: cluster-1 uniquely identify the membership of a cluster in a ClusterSet for the lifetime of the membership. apiVersion: about.k8s.io/v1 kind: ClusterProperty metadata: name: clusterset.k8s.io spec: value: mycoolclusterset Provide a reference point for multi-cluster tooling to build on within a cluster set, for example for DNS labels, for logging and tracing, etc. Provide extra metadata space to store other cluster properties that might otherwise be implemented as ad-hoc annotations on semantically adjacent objects. apiVersion: about.k8s.io/v1 kind: ClusterProperty metadata: name: fingerprint.mycoolimplementation.com spec: value: '{\"major\": \"1\",\"minor\": \"18\",\"gitVersion\": \"v1.18.2\",\"gitCommit\": \"52c56ce7a8272c798dbc29846288d7cd9fbae032\",\"gitTreeState\": \"clean\",\"buildDate\": \"2020-04-30T20:19:45Z\",\"goVersion\": \"go1.13.9\",\"compiler\": \"gc\",\"platform\": \"linux/amd64\"}'","title":"ClusterSet"},{"location":"api-types/cluster-set/#clusterset","text":"ClusterSet represents a specific pattern implemented by various organizations. A ClusterSet is typically: A group of clusters governed by a single authority. There is usually a high degree of trust within the set of clusters. Namespace Sameness applies to clusters in the set: Permissions and characteristics are consistent across clusters for a given namespace. Namespaces don't have to exist in every cluster, but behave the same across those in which they do. Note The early definition of the ClusterSet was described in KEP-2149 . It is now part of the About API . A cluster's ClusterSet membership is stored in the about.k8s.io/ClusterProperty clusterset.k8s.io .","title":"ClusterSet"},{"location":"api-types/cluster-set/#cluster-metadata","text":"The ClusterSet is a Cluster-scoped ClusterProperty CRD (Customer Resource Definition), that stores a name and a value. This property can be used to: uniquely identify clusters using a clusterID apiVersion: about.k8s.io/v1 kind: ClusterProperty metadata: name: cluster.clusterset.k8s.io spec: value: cluster-1 uniquely identify the membership of a cluster in a ClusterSet for the lifetime of the membership. apiVersion: about.k8s.io/v1 kind: ClusterProperty metadata: name: clusterset.k8s.io spec: value: mycoolclusterset Provide a reference point for multi-cluster tooling to build on within a cluster set, for example for DNS labels, for logging and tracing, etc. Provide extra metadata space to store other cluster properties that might otherwise be implemented as ad-hoc annotations on semantically adjacent objects. apiVersion: about.k8s.io/v1 kind: ClusterProperty metadata: name: fingerprint.mycoolimplementation.com spec: value: '{\"major\": \"1\",\"minor\": \"18\",\"gitVersion\": \"v1.18.2\",\"gitCommit\": \"52c56ce7a8272c798dbc29846288d7cd9fbae032\",\"gitTreeState\": \"clean\",\"buildDate\": \"2020-04-30T20:19:45Z\",\"goVersion\": \"go1.13.9\",\"compiler\": \"gc\",\"platform\": \"linux/amd64\"}'","title":"Cluster Metadata"},{"location":"api-types/service-export/","text":"ServiceExport \u00b6 Resource Definition \u00b6 A ServiceExport is a Customer Resource Definition used to specify which Kubernetes Services should be exported within a cluster. A ServiceExport resource is created with the cluster and namespace that a given Service resides in, and is name-mapped to the service for export. In other words, the ServiceExport is referenced with the same name as the export. If multiple clusters export a Service with the same namespaced name, they will be recognized as a single combined service. DNS \u00b6 When a ServiceExport is created, this will cause a domain name for the multi-cluster service to become accessible from within the ClusterSet. The domain name will be <service-export-name>.<service-export-namespace>.svc.clusterset.local . EndPointSlice \u00b6 When a ServiceExport is created, this will cause EndpointSlice objects for the underlying Service to be created in each cluster within the ClusterSet. One or more EndpointSlice resources will exist for each cluster that exported the Service, with each EndpointSlice containing only endpoints from its source cluster. These EndpointSlice objects are marked as managed by the ClusterSet service controller, so that the endpoint slice controller doesn\u2019t delete them.","title":"ServiceExport"},{"location":"api-types/service-export/#serviceexport","text":"","title":"ServiceExport"},{"location":"api-types/service-export/#resource-definition","text":"A ServiceExport is a Customer Resource Definition used to specify which Kubernetes Services should be exported within a cluster. A ServiceExport resource is created with the cluster and namespace that a given Service resides in, and is name-mapped to the service for export. In other words, the ServiceExport is referenced with the same name as the export. If multiple clusters export a Service with the same namespaced name, they will be recognized as a single combined service.","title":"Resource Definition"},{"location":"api-types/service-export/#dns","text":"When a ServiceExport is created, this will cause a domain name for the multi-cluster service to become accessible from within the ClusterSet. The domain name will be <service-export-name>.<service-export-namespace>.svc.clusterset.local .","title":"DNS"},{"location":"api-types/service-export/#endpointslice","text":"When a ServiceExport is created, this will cause EndpointSlice objects for the underlying Service to be created in each cluster within the ClusterSet. One or more EndpointSlice resources will exist for each cluster that exported the Service, with each EndpointSlice containing only endpoints from its source cluster. These EndpointSlice objects are marked as managed by the ClusterSet service controller, so that the endpoint slice controller doesn\u2019t delete them.","title":"EndPointSlice"},{"location":"api-types/service-import/","text":"ServiceImport \u00b6","title":"ServiceImport"},{"location":"api-types/service-import/#serviceimport","text":"","title":"ServiceImport"},{"location":"blog/","text":"Blog \u00b6 Archiving Kubefed on January 3rd, 2023 \u00b6 November 16, 2022 \u00b7 5 min read As discussed over the past few SIG meetings, Kubecon, and this list, KubeFed is heading for archival. We plan to create the tombstone commit and complete the process in seven weeks on Jan 3, 2023 so that there's time to get any last changes into the main repo. This is meant to clarify the state of the \u2018federation\u2019 concept and associated projects in Kubernetes and to better set expectations around development and support in the area. Archiving will enable us to send a clear signal about the direction the SIG is headed in and how we will approach our work. Continue reading","title":"Index"},{"location":"blog/#blog","text":"","title":"Blog"},{"location":"blog/#archiving-kubefed-on-january-3rd-2023","text":"November 16, 2022 \u00b7 5 min read As discussed over the past few SIG meetings, Kubecon, and this list, KubeFed is heading for archival. We plan to create the tombstone commit and complete the process in seven weeks on Jan 3, 2023 so that there's time to get any last changes into the main repo. This is meant to clarify the state of the \u2018federation\u2019 concept and associated projects in Kubernetes and to better set expectations around development and support in the area. Archiving will enable us to send a clear signal about the direction the SIG is headed in and how we will approach our work. Continue reading","title":"Archiving Kubefed on January 3rd, 2023"},{"location":"blog/2022/2022-11-16_archiving-kubefed-on-Jan-3-2023/","text":"November 16, 2022 \u00b7 5 min read As discussed over the past few SIG meetings, Kubecon, and this list, KubeFed is heading for archival. We plan to create the tombstone commit and complete the process in seven weeks on Jan 3, 2023 so that there's time to get any last changes into the main repo. This is meant to clarify the state of the \u2018federation\u2019 concept and associated projects in Kubernetes and to better set expectations around development and support in the area. Archiving will enable us to send a clear signal about the direction the SIG is headed in and how we will approach our work. Archival is not deletion and the code will remain on GitHub for reference or to fork and start your own projects. Nothing is going away and those who rely on kubefed can base new projects on the source, expanding and collaborating as you see fit. We want to thank everyone who has contributed to Kubefed over the past few years, it's been a huge effort from many people and has brought a ton of value to the community. We recognize and appreciate all of your hard work. While we don't have a SIG-endorsed replacement project and will not be linking to other projects from the tombstone, we will be linking to this thread. We welcome the community to chime in here with alternatives you're using or your own projects in the space. Thanks all! Jeremy Olmsted-Thompson Paul Morie SIG Multicluster Chairs","title":"Archiving Kubefed on Jan 3rd, 2023"},{"location":"concepts/about-api/","text":"About API Overview \u00b6","title":"About API Overview"},{"location":"concepts/about-api/#about-api-overview","text":"","title":"About API Overview"},{"location":"concepts/multicluster-services-api/","text":"Multicluster Services API Overview \u00b6 This document provides an overview of Multicluster Services API. This is an extension of the Services concept across multiple clusters. Services are the basic way that workloads communicate with each other in Kubernetes, and the Multicluster Services builds upon the Namespace Sameness concept to extend Services across multiclusters. In short, Services can remain available across clusters simply by using the same names. The Control Plane can be centralized or decentralized, but consumers only even rely on local data. This document solely focuses on the API and the common behaviour, leaving room for various implementations . There is no reference implementation available. The intent of the Multicluster Services API is that ClusterIP and headless services just work as expected across clusters. You can read more details about the API in the KEP-1645 . Multicluster API concepts \u00b6 Service and ServiceExport \u00b6 Main interaction point for user/administrator with MCS A custom resource you can create that marks a Service for export The mcs-controller consumes these Learn more at ServiceExport ServiceImport and EndpointSlices \u00b6 Created by the mcs-controller in all namespace-same clusters in the ClusterSet Representing the imported service and all the available backends for it across the ClusterSet Used to create the related EndpointSlices in consuming clusters Learn more at ServiceImport","title":"Multicluster Services API Overview"},{"location":"concepts/multicluster-services-api/#multicluster-services-api-overview","text":"This document provides an overview of Multicluster Services API. This is an extension of the Services concept across multiple clusters. Services are the basic way that workloads communicate with each other in Kubernetes, and the Multicluster Services builds upon the Namespace Sameness concept to extend Services across multiclusters. In short, Services can remain available across clusters simply by using the same names. The Control Plane can be centralized or decentralized, but consumers only even rely on local data. This document solely focuses on the API and the common behaviour, leaving room for various implementations . There is no reference implementation available. The intent of the Multicluster Services API is that ClusterIP and headless services just work as expected across clusters. You can read more details about the API in the KEP-1645 .","title":"Multicluster Services API Overview"},{"location":"concepts/multicluster-services-api/#multicluster-api-concepts","text":"","title":"Multicluster API concepts"},{"location":"concepts/multicluster-services-api/#service-and-serviceexport","text":"Main interaction point for user/administrator with MCS A custom resource you can create that marks a Service for export The mcs-controller consumes these Learn more at ServiceExport","title":"Service and ServiceExport"},{"location":"concepts/multicluster-services-api/#serviceimport-and-endpointslices","text":"Created by the mcs-controller in all namespace-same clusters in the ClusterSet Representing the imported service and all the available backends for it across the ClusterSet Used to create the related EndpointSlices in consuming clusters Learn more at ServiceImport","title":"ServiceImport and EndpointSlices"},{"location":"concepts/namespace-sameness/","text":"Namespace Sameness \u00b6 Permissions and characteristics are consistent across clusters for a given namespace. Namespaces don't have to exist in every cluster, but behave the same across those in which they do.","title":"Namespace Sameness"},{"location":"concepts/namespace-sameness/#namespace-sameness","text":"Permissions and characteristics are consistent across clusters for a given namespace. Namespaces don't have to exist in every cluster, but behave the same across those in which they do.","title":"Namespace Sameness"},{"location":"concepts/work-api/","text":"Work API Overview \u00b6 A common Work API to distribute workload to multiple clusters. Terminology \u00b6 Work Hub is a Kubernetes cluster where the Work API resource resides. Managed Cluster or Spoke Cluster is a Kubernetes cluster managed by the Work Hub. The workload resources defined in the Work API are applied on the managed cluster. Work Controller is a controller that reconciles the Work API resource on Work Hub, and applies resources defined in the Work to the Managed Cluster. Overview \u00b6 A Work is a custom resource that represent a list of API resources to be deployed on a cluster. The Work is created on the Work Hub, and resides in the namespace that the Work Controller is authorized to access. Creation of a Work on the Work Hub indicates that resources defined in the Work will be applied on a certain Managed Cluster. Update of a Work will trigger the resource update on the Managed Cluster, and deletion of a Work will garbage collect the resources on the Managed Cluster. Example \u00b6 apiVersion: multicluster.x-k8s.io/v1alpha1 kind: Work metadata: name: work-sample namespace: cluster1 spec: workload: manifests: - apiVersion: v1 kind: ConfigMap metadata: name: cm namespace: default data: ui.properties: | color=purple User creates a Work in the `cluster1 namespace on the Work Hub that the Work Controller is authorized to access. The Work Controller then accesses the Managed Cluster and applies the resources defined in the Work in its reconcile loop. The Work controller also tracks the status of applied resources by updating the Work status. Implementation \u00b6 This Work API project solely focuses on the API and the common behaviour, leaving room for various implementations. There is a reference implementation available in the GitHub repo . For more advanced Work API implementations: Open Cluster Management ManifestWork API Karmada Work API Azure Work API","title":"Work API Overview"},{"location":"concepts/work-api/#work-api-overview","text":"A common Work API to distribute workload to multiple clusters.","title":"Work API Overview"},{"location":"concepts/work-api/#terminology","text":"Work Hub is a Kubernetes cluster where the Work API resource resides. Managed Cluster or Spoke Cluster is a Kubernetes cluster managed by the Work Hub. The workload resources defined in the Work API are applied on the managed cluster. Work Controller is a controller that reconciles the Work API resource on Work Hub, and applies resources defined in the Work to the Managed Cluster.","title":"Terminology"},{"location":"concepts/work-api/#overview","text":"A Work is a custom resource that represent a list of API resources to be deployed on a cluster. The Work is created on the Work Hub, and resides in the namespace that the Work Controller is authorized to access. Creation of a Work on the Work Hub indicates that resources defined in the Work will be applied on a certain Managed Cluster. Update of a Work will trigger the resource update on the Managed Cluster, and deletion of a Work will garbage collect the resources on the Managed Cluster.","title":"Overview"},{"location":"concepts/work-api/#example","text":"apiVersion: multicluster.x-k8s.io/v1alpha1 kind: Work metadata: name: work-sample namespace: cluster1 spec: workload: manifests: - apiVersion: v1 kind: ConfigMap metadata: name: cm namespace: default data: ui.properties: | color=purple User creates a Work in the `cluster1 namespace on the Work Hub that the Work Controller is authorized to access. The Work Controller then accesses the Managed Cluster and applies the resources defined in the Work in its reconcile loop. The Work controller also tracks the status of applied resources by updating the Work status.","title":"Example"},{"location":"concepts/work-api/#implementation","text":"This Work API project solely focuses on the API and the common behaviour, leaving room for various implementations. There is a reference implementation available in the GitHub repo . For more advanced Work API implementations: Open Cluster Management ManifestWork API Karmada Work API Azure Work API","title":"Implementation"},{"location":"contributing/","text":"How to Get Involved \u00b6 This page contains links to all of the meeting notes, design docs and related discussions around the different APIs managed by the Multicluster SIG. Feedback and Questions \u00b6 For general feedback, questions or to share ideas please feel free to create a new discussion against the site repo . Bug Reports \u00b6 Bug reports should be filed as Github Issues on their respective subproject repo. Open an issue for a bug with the About API Open an issue for a bug with the MCS API Open an issue for a bug with the Work API NOTE : If you're reporting a bug that applies to a specific implementation of a SIG-MC sponsored API and not the API specification itself, please check our implementations page to find links to the repositories where you can get help with your specific implementation. Communications \u00b6 Major discussions and notifications will be sent on the SIG-MC mailing list . We also have a Slack channel (sig-multicluster) on k8s.io for day-to-day questions, discussions. Meetings \u00b6 Meetings discussing the evolution of the different APIs on SIG-Multicluster happen bi-weekly on Tuesdays at 9:30AM Pacific Time / 18:30 CET. Join kubernetes-sig-multicluster@googlegroups.com to get a calendar invite. Zoom link Convert to your timezone Add to your calendar Meeting Notes and Recordings \u00b6 Meeting agendas and notes are maintained in the meeting notes doc . Feel free to add topics for discussion at an upcoming meeting. All meetings are recorded and automatically uploaded to the SIG Multicluster meetings Youtube playlist . Archived Notes \u00b6 Some documents from previous quarters were uploaded here . Initial Design Discussions \u00b6 Presentations and Talks \u00b6 Date Title October, 2022 Kubecon NA 2022 Detroit: SIG Multicluster Intro & Deep Dive (AWS-based demo combining About API and their MCS implementation with AWS CloudMap Controller) slides , video October, 2022 Kubecon NA 2022 Detroit: Multi-Cluster Stateful Set Migration: A solution to Upgrade Pain slides , video May, 2022 Kubecon EU 2022 Valencia: SIG Multicluster Intro & Deep Dive (Demo on multicluster plugin for CoreDNS) video October, 2021 Kubecon NA 2021 Los Angeles: SIG Multicluster Intro & Deep Dive ] (Explanation of MCS, multicluster DNS) slides , video October, 2021 Kubecon NA 2021 Los Angeles: Here Be Services: Beyond the Cluster Boundary with Multicluster Services (Demo of MCS on GKE and Submariner.io) slides , video August, 2020 Kubecon EU 2020 Virtual : SIG Multicluster Intro video November, 2019 Kubecon 2019 San Diego: Intro + Deep Dive SIG Multicluster slides May, 2019 Kubecon 2019 Barcelona: Ingress V2 and Multicluster Services slides , video May, 2019 Kubecon 2019 Barcelona: Intro + Deep Dive: Multicluster SIG video Code of conduct \u00b6 Participation in the Kubernetes community is governed by the Kubernetes Code of Conduct","title":"How to Get Involved"},{"location":"contributing/#how-to-get-involved","text":"This page contains links to all of the meeting notes, design docs and related discussions around the different APIs managed by the Multicluster SIG.","title":"How to Get Involved"},{"location":"contributing/#feedback-and-questions","text":"For general feedback, questions or to share ideas please feel free to create a new discussion against the site repo .","title":"Feedback and Questions"},{"location":"contributing/#bug-reports","text":"Bug reports should be filed as Github Issues on their respective subproject repo. Open an issue for a bug with the About API Open an issue for a bug with the MCS API Open an issue for a bug with the Work API NOTE : If you're reporting a bug that applies to a specific implementation of a SIG-MC sponsored API and not the API specification itself, please check our implementations page to find links to the repositories where you can get help with your specific implementation.","title":"Bug Reports"},{"location":"contributing/#communications","text":"Major discussions and notifications will be sent on the SIG-MC mailing list . We also have a Slack channel (sig-multicluster) on k8s.io for day-to-day questions, discussions.","title":"Communications"},{"location":"contributing/#meetings","text":"Meetings discussing the evolution of the different APIs on SIG-Multicluster happen bi-weekly on Tuesdays at 9:30AM Pacific Time / 18:30 CET. Join kubernetes-sig-multicluster@googlegroups.com to get a calendar invite. Zoom link Convert to your timezone Add to your calendar","title":"Meetings"},{"location":"contributing/#meeting-notes-and-recordings","text":"Meeting agendas and notes are maintained in the meeting notes doc . Feel free to add topics for discussion at an upcoming meeting. All meetings are recorded and automatically uploaded to the SIG Multicluster meetings Youtube playlist .","title":"Meeting Notes and Recordings"},{"location":"contributing/#archived-notes","text":"Some documents from previous quarters were uploaded here .","title":"Archived Notes"},{"location":"contributing/#initial-design-discussions","text":"","title":"Initial Design Discussions"},{"location":"contributing/#presentations-and-talks","text":"Date Title October, 2022 Kubecon NA 2022 Detroit: SIG Multicluster Intro & Deep Dive (AWS-based demo combining About API and their MCS implementation with AWS CloudMap Controller) slides , video October, 2022 Kubecon NA 2022 Detroit: Multi-Cluster Stateful Set Migration: A solution to Upgrade Pain slides , video May, 2022 Kubecon EU 2022 Valencia: SIG Multicluster Intro & Deep Dive (Demo on multicluster plugin for CoreDNS) video October, 2021 Kubecon NA 2021 Los Angeles: SIG Multicluster Intro & Deep Dive ] (Explanation of MCS, multicluster DNS) slides , video October, 2021 Kubecon NA 2021 Los Angeles: Here Be Services: Beyond the Cluster Boundary with Multicluster Services (Demo of MCS on GKE and Submariner.io) slides , video August, 2020 Kubecon EU 2020 Virtual : SIG Multicluster Intro video November, 2019 Kubecon 2019 San Diego: Intro + Deep Dive SIG Multicluster slides May, 2019 Kubecon 2019 Barcelona: Ingress V2 and Multicluster Services slides , video May, 2019 Kubecon 2019 Barcelona: Intro + Deep Dive: Multicluster SIG video","title":"Presentations and Talks"},{"location":"contributing/#code-of-conduct","text":"Participation in the Kubernetes community is governed by the Kubernetes Code of Conduct","title":"Code of conduct"},{"location":"contributing/devguide/","text":"","title":"Developer Guide"},{"location":"contributing/faq/","text":"Frequently Asked Questions (FAQ) \u00b6 Q: How can I get involved with Multicluster API? A: The community page keeps track of how to get involved with the project. Q: Where can I find Multicluster API releases? A: Multicluster API releases are tags of the Github repository . The Github releases page shows all the releases. Q: Which Kubernetes versions are supported? A: Generally, the About and Multicluster Services APIs support Kubernetes 1.21+ as the Multicluster API requires support for the Kubernetes EndpointSlices .","title":"FAQ"},{"location":"contributing/faq/#frequently-asked-questions-faq","text":"Q: How can I get involved with Multicluster API? A: The community page keeps track of how to get involved with the project. Q: Where can I find Multicluster API releases? A: Multicluster API releases are tags of the Github repository . The Github releases page shows all the releases. Q: Which Kubernetes versions are supported? A: Generally, the About and Multicluster Services APIs support Kubernetes 1.21+ as the Multicluster API requires support for the Kubernetes EndpointSlices .","title":"Frequently Asked Questions (FAQ)"},{"location":"guides/","text":"Implementations \u00b6 This document tracks downstream implementations and integrations of Multicluster API and provides status and resource references for them. Implementors and integrators of Multicluster API are encouraged to update this document with status information about their implementations, the versions they cover, and documentation to help users get started. Implementation Status \u00b6 Google Cloud MCS (General Availability) Submariner (???) Implementations \u00b6 In this section you will find specific links to blog posts, documentation and other Multicluster API references for specific implementations. Google Kubernetes Engine \u00b6 Google Kubernetes Engine (GKE) is a managed Kubernetes platform offered by Google Cloud. GKE's implementation of the Multicluster API is through the GKE Multi Cluster Service . Please follow this guide for the first steps to set up multicluster services on GKE. Submariner \u00b6 Submariner is an open-source project enabling direct networking between Pods and Services in different Kubernetes clusters, either on-premises or in the cloud. Submariner provides: Cross-cluster L3 connectivity using encrypted and unencrypted connections Service Discovery across clusters subctl , a command-line utility that simplifies deployment and management Support for interconnecting clusters using overlapping CIDRs","title":"Index"},{"location":"guides/#implementations","text":"This document tracks downstream implementations and integrations of Multicluster API and provides status and resource references for them. Implementors and integrators of Multicluster API are encouraged to update this document with status information about their implementations, the versions they cover, and documentation to help users get started.","title":"Implementations"},{"location":"guides/#implementation-status","text":"Google Cloud MCS (General Availability) Submariner (???)","title":"Implementation Status"},{"location":"guides/#implementations_1","text":"In this section you will find specific links to blog posts, documentation and other Multicluster API references for specific implementations.","title":"Implementations"},{"location":"guides/#google-kubernetes-engine","text":"Google Kubernetes Engine (GKE) is a managed Kubernetes platform offered by Google Cloud. GKE's implementation of the Multicluster API is through the GKE Multi Cluster Service . Please follow this guide for the first steps to set up multicluster services on GKE.","title":"Google Kubernetes Engine"},{"location":"guides/#submariner","text":"Submariner is an open-source project enabling direct networking between Pods and Services in different Kubernetes clusters, either on-premises or in the cloud. Submariner provides: Cross-cluster L3 connectivity using encrypted and unencrypted connections Service Discovery across clusters subctl , a command-line utility that simplifies deployment and management Support for interconnecting clusters using overlapping CIDRs","title":"Submariner"},{"location":"guides/gateway-api/","text":"Multicluster API support in Gateway API \u00b6","title":"Gateway API"},{"location":"guides/gateway-api/#multicluster-api-support-in-gateway-api","text":"","title":"Multicluster API support in Gateway API"},{"location":"guides/gke-mcs/","text":"Getting started with GKE Multicluster Services \u00b6","title":"GKE Multicluster Services"},{"location":"guides/gke-mcs/#getting-started-with-gke-multicluster-services","text":"","title":"Getting started with GKE Multicluster Services"},{"location":"guides/guidelines/","text":"Implementation guidelines \u00b6 There are some general design guidelines used throughout these APIs. Note Throughout the Multicluster API documentation and specification, keywords such as \"MUST\", \"MAY\", and \"SHOULD\" are used broadly. These should be interpreted as described in RFC 2119. Single resource consistency \u00b6 The Kubernetes API guarantees consistency only on a single resource level. There are a couple of consequences for complex resource graphs as opposed to single resources: Error checking of properties spanning multiple resource will be asynchronous and eventually consistent. Simple syntax checks will be possible at the single resource level, but cross resource dependencies will need to be handled by the controller. Controllers will need to handle broken links between resources and/or mismatched configuration. Conflicts \u00b6 Separation and delegation of responsibility among independent actors (e.g between cluster ops and application developers) can result in conflicts in the configuration. For example, two application teams may inadvertently submit configuration for the same HTTP path. In most cases, guidance for conflict resolution is provided along with the documentation for fields that may have a conflict. If a conflict does not have a prescribed resolution, the following guiding principles should be applied: Prefer not to break things that are working. Drop as little traffic as possible. Provide a consistent experience when conflicts occur. Make it clear which path has been chosen when a conflict has been identified. Where possible, this should be communicated by setting appropriate status conditions on relevant resources. More specific matches should be given precedence over less specific ones. The resource with the oldest creation timestamp wins. If everything else is equivalent (including creation timestamp), precedences should be given to the resource appearing first in alphabetical order (namespace/name). For example, foo/bar would be given precedence over foo/baz. Gracefully Handling Future API Versions \u00b6 An important consideration when implementing this API is how it might change in the future. Similar to the Ingress API before it, this API is designed to be implemented by a variety of different products within the same cluster. That means that the API version your implementation was developed with may be different than the API version it is used with. At a minimum, the following requirements must be met to ensure future versions of the API do not break your implementation: Handle fields with loosened validation without crashing Handle fields that have transitioned from required to optional without crashing Limitations of CRD and Webhook Validation \u00b6 CRD and webhook validation is not the final validation i.e. webhook is \"nice UX\" but not schema enforcement. This validation is intended to provide immediate feedback to users when they provide an invalid configuration. Write code defensively with the assumption that at least some invalid input (Multicluster/About/Work API resources) will reach your controller. Both Webhook and CRD validation is not fully reliable because it: May not be deployed correctly. May be loosened in future API releases. (Fields may contain values with less restrictive validation in newer versions of the API). Note: These limitations are not unique to Multicluster API and apply more broadly to any Kubernetes CRDs and webhooks. Implementers should ensure that, even if unexpected values are encountered in the API, their implementations are still as secure as possible and handle this input gracefully. The most common response would be to reject the configuration as malformed and signal the user via a condition in the status block. To avoid duplicating work, Multicluster API maintainers are considering adding a shared validation package that implementations can use for this purpose. Expectations \u00b6 We expect there will be varying levels of conformance among the different providers in the early days of this API. Users can use the results of the conformance tests to understand areas where there may be differences in behavior from the spec. Implementation-specific \u00b6 In some aspects of the APIs, we give the user an ability to specify usage of the feature, however, the exact behavior may depend on the underlying implementation. For example, regular expression matching is present in all implementations but specifying an exact behavior is impossible due to subtle differences between the underlying libraries used (e.g. PCRE, ECMA, Re2). It is still useful for our users to spec out the feature as much as possible, but we acknowledge that the behavior for some subset of the API may still vary (and that's ok). These cases will be specified as defining delimited parts of the API \"implementation-specific\". Kind vs. Resource \u00b6 Similar to other Kubernetes APIs, Multicluster, Work and About APIs use \"Kind\" instead of \"Resource\" in object references throughout the API. This pattern should be familiar to most Kubernetes users. Per the [Kubernetes API conventions][1], this means that all implementations of this API should have a predefined mapping between kinds and resources. Relying on dynamic resource mapping is not safe. API Conventions \u00b6 The Multicluster, About and Work APIs follow Kubernetes API [conventions][1]. These conventions are intended to ease client development and ensure that configuration mechanisms can consistently be implemented across a diverse set of use cases.","title":"Implementation Guidelines"},{"location":"guides/guidelines/#implementation-guidelines","text":"There are some general design guidelines used throughout these APIs. Note Throughout the Multicluster API documentation and specification, keywords such as \"MUST\", \"MAY\", and \"SHOULD\" are used broadly. These should be interpreted as described in RFC 2119.","title":"Implementation guidelines"},{"location":"guides/guidelines/#single-resource-consistency","text":"The Kubernetes API guarantees consistency only on a single resource level. There are a couple of consequences for complex resource graphs as opposed to single resources: Error checking of properties spanning multiple resource will be asynchronous and eventually consistent. Simple syntax checks will be possible at the single resource level, but cross resource dependencies will need to be handled by the controller. Controllers will need to handle broken links between resources and/or mismatched configuration.","title":"Single resource consistency"},{"location":"guides/guidelines/#conflicts","text":"Separation and delegation of responsibility among independent actors (e.g between cluster ops and application developers) can result in conflicts in the configuration. For example, two application teams may inadvertently submit configuration for the same HTTP path. In most cases, guidance for conflict resolution is provided along with the documentation for fields that may have a conflict. If a conflict does not have a prescribed resolution, the following guiding principles should be applied: Prefer not to break things that are working. Drop as little traffic as possible. Provide a consistent experience when conflicts occur. Make it clear which path has been chosen when a conflict has been identified. Where possible, this should be communicated by setting appropriate status conditions on relevant resources. More specific matches should be given precedence over less specific ones. The resource with the oldest creation timestamp wins. If everything else is equivalent (including creation timestamp), precedences should be given to the resource appearing first in alphabetical order (namespace/name). For example, foo/bar would be given precedence over foo/baz.","title":"Conflicts"},{"location":"guides/guidelines/#gracefully-handling-future-api-versions","text":"An important consideration when implementing this API is how it might change in the future. Similar to the Ingress API before it, this API is designed to be implemented by a variety of different products within the same cluster. That means that the API version your implementation was developed with may be different than the API version it is used with. At a minimum, the following requirements must be met to ensure future versions of the API do not break your implementation: Handle fields with loosened validation without crashing Handle fields that have transitioned from required to optional without crashing","title":"Gracefully Handling Future API Versions"},{"location":"guides/guidelines/#limitations-of-crd-and-webhook-validation","text":"CRD and webhook validation is not the final validation i.e. webhook is \"nice UX\" but not schema enforcement. This validation is intended to provide immediate feedback to users when they provide an invalid configuration. Write code defensively with the assumption that at least some invalid input (Multicluster/About/Work API resources) will reach your controller. Both Webhook and CRD validation is not fully reliable because it: May not be deployed correctly. May be loosened in future API releases. (Fields may contain values with less restrictive validation in newer versions of the API). Note: These limitations are not unique to Multicluster API and apply more broadly to any Kubernetes CRDs and webhooks. Implementers should ensure that, even if unexpected values are encountered in the API, their implementations are still as secure as possible and handle this input gracefully. The most common response would be to reject the configuration as malformed and signal the user via a condition in the status block. To avoid duplicating work, Multicluster API maintainers are considering adding a shared validation package that implementations can use for this purpose.","title":"Limitations of CRD and Webhook Validation"},{"location":"guides/guidelines/#expectations","text":"We expect there will be varying levels of conformance among the different providers in the early days of this API. Users can use the results of the conformance tests to understand areas where there may be differences in behavior from the spec.","title":"Expectations"},{"location":"guides/guidelines/#implementation-specific","text":"In some aspects of the APIs, we give the user an ability to specify usage of the feature, however, the exact behavior may depend on the underlying implementation. For example, regular expression matching is present in all implementations but specifying an exact behavior is impossible due to subtle differences between the underlying libraries used (e.g. PCRE, ECMA, Re2). It is still useful for our users to spec out the feature as much as possible, but we acknowledge that the behavior for some subset of the API may still vary (and that's ok). These cases will be specified as defining delimited parts of the API \"implementation-specific\".","title":"Implementation-specific"},{"location":"guides/guidelines/#kind-vs-resource","text":"Similar to other Kubernetes APIs, Multicluster, Work and About APIs use \"Kind\" instead of \"Resource\" in object references throughout the API. This pattern should be familiar to most Kubernetes users. Per the [Kubernetes API conventions][1], this means that all implementations of this API should have a predefined mapping between kinds and resources. Relying on dynamic resource mapping is not safe.","title":"Kind vs. Resource"},{"location":"guides/guidelines/#api-conventions","text":"The Multicluster, About and Work APIs follow Kubernetes API [conventions][1]. These conventions are intended to ease client development and ensure that configuration mechanisms can consistently be implemented across a diverse set of use cases.","title":"API Conventions"},{"location":"guides/istio/","text":"Multicluster API support in Istio \u00b6","title":"Istio"},{"location":"guides/istio/#multicluster-api-support-in-istio","text":"","title":"Multicluster API support in Istio"},{"location":"guides/submariner-mcs/","text":"Getting started with Submariner Multicluster Services \u00b6","title":"Submariner"},{"location":"guides/submariner-mcs/#getting-started-with-submariner-multicluster-services","text":"","title":"Getting started with Submariner Multicluster Services"},{"location":"references/conformance/","text":"Conformance \u00b6 This API covers a broad set of features and use cases and has been implemented widely. This combination of both a large feature set and variety of implementations requires clear conformance definitions and tests to ensure the API provides a consistent experience wherever it is used. When considering Multicluster API conformance, there are three important concepts: 1. Release Channels \u00b6 2. Support Levels \u00b6 Overlapping Support Levels \u00b6 3. Conformance Tests \u00b6 Running Tests \u00b6 Contributing to Conformance \u00b6","title":"Conformance"},{"location":"references/conformance/#conformance","text":"This API covers a broad set of features and use cases and has been implemented widely. This combination of both a large feature set and variety of implementations requires clear conformance definitions and tests to ensure the API provides a consistent experience wherever it is used. When considering Multicluster API conformance, there are three important concepts:","title":"Conformance"},{"location":"references/conformance/#1-release-channels","text":"","title":"1. Release Channels"},{"location":"references/conformance/#2-support-levels","text":"","title":"2. Support Levels"},{"location":"references/conformance/#overlapping-support-levels","text":"","title":"Overlapping Support Levels"},{"location":"references/conformance/#3-conformance-tests","text":"","title":"3. Conformance Tests"},{"location":"references/conformance/#running-tests","text":"","title":"Running Tests"},{"location":"references/conformance/#contributing-to-conformance","text":"","title":"Contributing to Conformance"},{"location":"references/security-model/","text":"Security Model \u00b6 Introduction \u00b6 Resources \u00b6 Roles and personas \u00b6 Advanced Concepts \u00b6","title":"Security Model"},{"location":"references/security-model/#security-model","text":"","title":"Security Model"},{"location":"references/security-model/#introduction","text":"","title":"Introduction"},{"location":"references/security-model/#resources","text":"","title":"Resources"},{"location":"references/security-model/#roles-and-personas","text":"","title":"Roles and personas"},{"location":"references/security-model/#advanced-concepts","text":"","title":"Advanced Concepts"},{"location":"references/spec/","text":"API Specification \u00b6 REPLACE_WITH_GENERATED_CONTENT","title":"API specification"},{"location":"references/spec/#api-specification","text":"REPLACE_WITH_GENERATED_CONTENT","title":"API Specification"}]}